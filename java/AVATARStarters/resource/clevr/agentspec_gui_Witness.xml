<AsapVirtualHuman id="CleVR">
  <Loader id="realizer" loader="asap.realizerembodiments.AsapRealizerEmbodiment">
    <BMLParser>
      <BMLAttributeExtension class="asap.bml.ext.bmla.BMLABMLBehaviorAttributes"/>      
    </BMLParser>
    <BMLScheduler>	
      <SchedulingHandler class="asap.realizer.scheduler.BMLASchedulingHandler" schedulingStrategy="asap.realizer.scheduler.SortedSmartBodySchedulingStrategy"/>
    </BMLScheduler>
	<!-- This is for BML requests through Apollo/middleware: -->
    <PipeLoader id="pipe" loader="asap.middlewareadapters.loader.MiddlewareToBMLRealizerAdapterLoader">
      <MiddlewareOptions loaderclass="nl.utwente.hmi.middleware.stomp.STOMPMiddlewareLoader">
        <MiddlewareProperty name="apolloIP" value="127.0.0.1"/>
        <MiddlewareProperty name="apolloPort" value="61613"/>
        <MiddlewareProperty name="iTopic" value="/topic/bmlRequests"/>
        <MiddlewareProperty name="oTopic" value="/topic/bmlFeedback"/>
      </MiddlewareOptions>
    </PipeLoader>
  </Loader>
  
  <Loader id="guiembodiment" loader="asap.realizerembodiments.JFrameEmbodiment">
    <BmlUI demoscriptresources="clevr/testbml"/>
    <FeedbackUI/>    
  </Loader>

  <Loader id="middlewareembodiment" loader="asap.middlewareengine.embodiment.MiddlewareEmbodiment" requiredloaders="">
	<!-- This is for behavior (gaze, gestures) that is controlled by CleVR in unity: -->
    <MiddlewareOptions loaderclass="nl.utwente.hmi.middleware.stomp.STOMPMiddlewareLoader">
      <MiddlewareProperty name="default" value="true"/>
      <MiddlewareProperty name="apolloIP" value="127.0.0.1"/>
      <MiddlewareProperty name="apolloPort" value="61613"/>
      <MiddlewareProperty name="iTopic" value="/topic/CleVRFeedback"/>
      <MiddlewareProperty name="oTopic" value="/topic/CleVRCmd"/>
    </MiddlewareOptions>
  </Loader>

  <Loader id="middlewareengine" loader="asap.middlewareengine.engine.MiddlewareEngineLoader" requiredloaders="middlewareembodiment">
    <MiddlewareBinding basedir="" resources="clevr/gestures" filename="middlewarebinding.xml"/>
  </Loader>
  

  
  <Loader id="unityembodiment" loader="hmi.unityembodiments.UnityEmbodimentLoader">
	<!-- This is for agent behavior (facial expressions & lipsync) controlled by ASAP: -->
    <MiddlewareOptions loaderclass="nl.utwente.hmi.middleware.stomp.STOMPMiddlewareLoader">
      <MiddlewareProperty name="apolloIP" value="127.0.0.1"/>
      <MiddlewareProperty name="apolloPort" value="61613"/>
      <MiddlewareProperty name="iTopic" value="/topic/UnityAgentFeedback"/>
      <MiddlewareProperty name="oTopic" value="/topic/UnityAgentControl"/>
    </MiddlewareOptions>
  </Loader>

  <Loader id="faceengine" 
          loader="asap.faceengine.loader.FaceEngineLoader"
          requiredloaders="unityembodiment">
    <!-- If you want to use FACS, but have an unusual facial rig (as most game rigs are), you can use the facs2morphmapping to
		map FACS to morphs (which can, on the unity side, be realized as a combination of morphs, bones, blendshapes... -->
    <FaceBinding basedir="" resources="clevr/face" filename="facebinding.xml" facs2morphmappingresources="clevr/face" facs2morphmappingfilename="facs2morphmapping.xml"/>
  </Loader>
  
  <Loader id="facelipsync" requiredloaders="faceengine" loader="asap.faceengine.loader.TimedFaceUnitLipSynchProviderLoader">
	<!-- binds a morphtarget (unity) to a viseme in the used set (see PhonemeToVisemeMapping) -->
	<MorphVisemeBinding resources="clevr/face" filename="visemebinding.xml"/>
  </Loader>
  
  <Loader id="clevrsubtitles" loader="hmi.AVATAR.CleVRSubtitlesLoader" requiredloaders="middlewareembodiment" />
  <Loader id="textengine" loader="asap.textengine.loader.TextEngineLoader" requiredloaders="clevrsubtitles" />
  <Loader id="subtitles" requiredloaders="textengine" loader="asap.textengine.loader.TimedSpeechTextUnitLipSynchProviderLoader" />
  
  
<!-- MARY TTS 
  <Loader id="ttsbinding" loader="asap.marytts5binding.loader.MaryTTSBindingLoader">
    <PhonemeToVisemeMapping resources="shared/face/phoneme2viseme" filename="sampaen2disney.xml"/>
  </Loader>

  <Loader id="speechengine" loader="asap.speechengine.loader.SpeechEngineLoader" requiredloaders="facelipsync,subtitles,ttsbinding,guiembodiment">
    <Voice factory="WAV_TTS" voicename="cmu-rms-hsmm" />
    <SpeechUI/>
  </Loader>
 /MARY TTS -->
<!-- FLUENCY TTS -
- /FLUENCY TTS -->
  <Loader id="ttsbinding" loader="asap.fluencyttsbinding.loader.FluencyTTSBindingLoader">
    <PhonemeToVisemeMapping resources="shared/face/phoneme2viseme" filename="fluency2disney.xml"/>
  </Loader>

  <Loader id="speechengine" loader="asap.speechengine.loader.SpeechEngineLoader" requiredloaders="facelipsync,subtitles,ttsbinding,guiembodiment">
    <Voice factory="WAV_TTS" voicename="Arthur"/>	
    <SpeechUI/>
  </Loader>

<!--
  <Loader id="blinkengine" loader="asap.emitterengine.loader.EmitterEngineLoader">
    <EmitterInfo class="asap.blinkemitter.BlinkEmitterInfo"/>
  </Loader>
-->
       
  <BMLRouting>
	<!-- These route behaviors to a certain engine, i.e., if a <gesture> tag is encountered,
		 the parsed behavior will be sent to the engine specified under ...GestureBehavior -->
		 
    <!-- <Route behaviourclass="saiba.bml.core.SpeechBehaviour" engineid="textengine"/> -->
    <Route behaviourclass="saiba.bml.core.GestureBehaviour" engineid="middlewareengine"/>
    <Route behaviourclass="saiba.bml.core.PostureBehaviour" engineid="middlewareengine"/>
    <Route behaviourclass="saiba.bml.core.HeadBehaviour" engineid="middlewareengine"/>
    <Route behaviourclass="saiba.bml.core.GazeBehaviour" engineid="middlewareengine"/>
    <Route behaviourclass="saiba.bml.core.PointingBehaviour" engineid="middlewareengine"/>
  </BMLRouting>
</AsapVirtualHuman>
